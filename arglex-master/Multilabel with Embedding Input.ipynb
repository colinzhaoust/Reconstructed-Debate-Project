{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish User loading, with number of users: 45348\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch import optim\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "from arglex import Classifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "\n",
    "arglex = Classifier()\n",
    "\n",
    "with open(\"users.json\", \"r\") as f:\n",
    "    users = json.load(f)\n",
    "\n",
    "number_of_users = len(users)\n",
    "print(\"Finish User loading, with number of users:\", number_of_users)\n",
    "# print(users.keys())\n",
    "# print(users[\"ahuggies30\"])\n",
    "get_big_issues = users[\"ahuggies30\"][\"big_issues_dict\"].keys()\n",
    "\n",
    "big_issues_list = []\n",
    "\n",
    "for item in get_big_issues:\n",
    "    big_issues_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################\n",
    "# # Load Word Embedding\n",
    "# ############################################\n",
    "\n",
    "\n",
    "# def sent_shuffle(sent):\n",
    "#     all_sents = []\n",
    "#     total_length = len(sent)\n",
    "\n",
    "#     for i in range(10):\n",
    "#         random.shuffle(sent)\n",
    "#         all_sents.append(sent[:])\n",
    "\n",
    "#     return all_sents\n",
    "\n",
    "\n",
    "# # sent = [\"I\", \"am\", \"what\", \"you\", \"don't\", \"expect\"]\n",
    "# # all_sents = sent_shuffle(sent)\n",
    "# # print()\n",
    "# # print(all_sents)\n",
    "# # breakpoint()\n",
    "\n",
    "# with open(\"users.json\", \"r\") as f:\n",
    "#     users = json.load(f)\n",
    "\n",
    "# sentences = []\n",
    "# # useful_aspects = [ 'education', 'ethnicity', 'income', 'interested', 'looking', 'number_of_friends',  'party', 'political_ideology', 'president', 'relationship', 'religious_ideology', 'win_ratio', 'gender']\n",
    "# # useful_aspects = ['political_ideology', 'education', 'ethnicity', 'interested', 'gender',\"religious_ideology\"]\n",
    "# useful_aspects = ['political_ideology','education', 'gender', \"religious_ideology\", \"party\", \"ethnicity\"]\n",
    "\n",
    "# for user in users.values():\n",
    "#     user_sent = []\n",
    "\n",
    "#     for aspect in useful_aspects:\n",
    "\n",
    "#         if user[aspect] == \"Not Saying\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             # user_sent.append(aspect + \":\" + user[aspect])\n",
    "#             user_sent.append(user[aspect])\n",
    "\n",
    "#     for key in user[\"big_issues_dict\"].keys():\n",
    "\n",
    "#         for choice in [\"Pro\", \"Con\"]: # , \"Und\", \"N/O\"]:\n",
    "#             this_sent = user_sent\n",
    "#             if user[\"big_issues_dict\"][key] == choice:\n",
    "#                 this_sent.append(key + \"-\" + choice)\n",
    "#                 if len(this_sent) > 0:\n",
    "#                     sent_list = this_sent\n",
    "#                     if len(sent_list) > 0:\n",
    "#                         sentences.extend(sent_list)\n",
    "\n",
    "\n",
    "# print(\"Total number of sentences generated\", len(sentences))\n",
    "\n",
    "# language_model = Word2Vec(sentences, size=8, window=20, sg=0)\n",
    "# # language_model = Word2Vec.load(\"user_aspect_embedding.bin\")\n",
    "# print(\"Finish loading the word vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# language_model.wv.save(\"user_ascpect_embedding.bin\")\n",
    "\n",
    "language_model = Word2Vec.load(\"user_aspect_embedding.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "\n",
    "def target_counter(target_list, users):\n",
    "    target_counters = dict()\n",
    "    for target in target_list:\n",
    "        target_counters[target] = dict()\n",
    "\n",
    "    for user in users.values():\n",
    "        for target in target_list:\n",
    "            if not target_counters[target].get(user[target]):\n",
    "                if \"Christian\" in  user[target]:\n",
    "                    user[target] = \"Christian\"\n",
    "                target_counters[target][user[target]] = 1\n",
    "            else:\n",
    "                if \"Christian\" in  user[target]:\n",
    "                    user[target] = \"Christian\"\n",
    "                target_counters[target][user[target]] += 1\n",
    "\n",
    "    return target_counters\n",
    "\n",
    "\n",
    "def criterion_generator(target_list, target_counters, bar):\n",
    "    criterion = dict()\n",
    "\n",
    "    for target in target_list:\n",
    "        criterion[target] = []\n",
    "        for item in target_counters[target].keys():\n",
    "            if item == \"Not Saying\":\n",
    "                continue\n",
    "            if target_counters[target][item] > bar:\n",
    "                criterion[target].append(item)\n",
    "        if len(criterion[target]) == 0:\n",
    "            del criterion[target]\n",
    "\n",
    "    return criterion\n",
    "\n",
    "religious_ideology = \"religious_ideology\"  # Christian or Atheist\n",
    "political_ideology = \"political_ideology\"  # Liberal or Conservative\n",
    "\n",
    "\n",
    "def user_filter(users, target_list, bar):\n",
    "    target_counters = target_counter(target_list, users)\n",
    "    criterion = criterion_generator(target_list, target_counters, bar)\n",
    "\n",
    "    valid_users = []\n",
    "    for user in users.values():\n",
    "        indicator = 1\n",
    "\n",
    "        for key in criterion.keys():\n",
    "            if not user.get(key):\n",
    "                indicator = 0\n",
    "                break\n",
    "            elif user.get(key) not in criterion[key]:\n",
    "                indicator = 0\n",
    "                break\n",
    "\n",
    "        if indicator:\n",
    "            valid_users.append(user)\n",
    "\n",
    "    # print(\"Number of valid users:\", len(valid_users))\n",
    "    return valid_users, criterion\n",
    "\n",
    "number = 0\n",
    "\n",
    "\n",
    "def get_user_aspect_vec(user, criterion, model, target_big_issues):\n",
    "    user_aspect_vec = []\n",
    "    for key in criterion.keys():\n",
    "        init = [0.0 for i in range(len(criterion[key]))]\n",
    "\n",
    "        if user[key] in criterion[key]:\n",
    "            index = criterion[key].index(user[key])\n",
    "            init[index] = 1.0\n",
    "        \n",
    "        # one-hot\n",
    "        user_aspect_vec.extend(init)\n",
    "\n",
    "        # Embedding\n",
    "        w2v_vec = model.wv[user[key]]\n",
    "        user_aspect_vec.extend(w2v_vec)\n",
    "\n",
    "        # Similarity\n",
    "        sim_pro_sum = 0\n",
    "        sim_con_sum = 0\n",
    "        sim_pro_mul = 0\n",
    "        sim_con_mul = 0\n",
    "\n",
    "        for target_big_issue in target_big_issues:\n",
    "            # choice = user[\"big_issues_dict\"][target_big_issue]\n",
    "            try:\n",
    "                # w2v_vec_issue = model.wv[target_big_issue + \"-\" + choice]\n",
    "                choice1 = \"Pro\"\n",
    "                choice2 = \"Con\"\n",
    "                sim_pro = model.wv.similarity(user[key], target_big_issue + \"-\" + choice1)\n",
    "                sim_con = model.wv.similarity(user[key], target_big_issue + \"-\" + choice2)\n",
    "                # user_aspect_vec.append(sim_pro)\n",
    "                # user_aspect_vec.append(sim_con)\n",
    "                # user_aspect_vec.append(sim_pro/sim_con)\n",
    "                # user_aspect_vec.append(sim_pro - sim_con)\n",
    "                sim_pro_sum += sim_pro\n",
    "                sim_con_sum += sim_con\n",
    "                if sim_pro == 0:\n",
    "                    sim_pro = 0.0001\n",
    "                if sim_con == 0:\n",
    "                    sim_con = 0.0001\n",
    "                sim_pro_mul *= sim_pro\n",
    "                sim_con_mul *= sim_con\n",
    "            except:\n",
    "                global number\n",
    "                number += 1\n",
    "                print(\"Number of missing in similarity part\", number)\n",
    "                user_aspect_vec.extend(np.array(w2v_vec) - np.array(w2v_vec_issue))\n",
    "\n",
    "    user_aspect_vec.append(sim_pro_sum)\n",
    "    user_aspect_vec.append(sim_con_sum)\n",
    "    user_aspect_vec.append(sim_pro_sum/sim_con_sum)\n",
    "    user_aspect_vec.append(sim_pro_sum - sim_con_sum)\n",
    "\n",
    "    return user_aspect_vec\n",
    "\n",
    "\n",
    "def language_feature_generator(text):\n",
    "\n",
    "    zero = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "    if len(text) == 0:\n",
    "        return zero\n",
    "\n",
    "    sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "    average_sent_length = 0.0\n",
    "    average_sent_subjectivity = 0.0\n",
    "    average_sent_polarity = 0.0\n",
    "\n",
    "    count = 0\n",
    "    for sentence in sent_tokenize_list:\n",
    "        count += 1\n",
    "        sent_blob = TextBlob(sentence)\n",
    "        average_sent_subjectivity += sent_blob.sentiment.subjectivity\n",
    "        average_sent_polarity += sent_blob.sentiment.polarity\n",
    "        average_sent_length += len(word_tokenize(sentence))\n",
    "\n",
    "    if count == 0:\n",
    "        return zero\n",
    "\n",
    "    average_sent_length = float(average_sent_length / count)\n",
    "    average_sent_subjectivity = float(average_sent_subjectivity / count)\n",
    "    average_sent_polarity = float(average_sent_polarity / count)\n",
    "\n",
    "    return len(sent_tokenize(text)), average_sent_length, average_sent_subjectivity, average_sent_polarity\n",
    "\n",
    "\n",
    "def get_user_language_vec(user):\n",
    "    language_vec = []\n",
    "\n",
    "    if not user.get(\"opinion_arguments\"):\n",
    "        return [0.0 for i in range(9)]\n",
    "\n",
    "    arguments = user[\"opinion_arguments\"]\n",
    "    # number of opinions\n",
    "    # language_vec.append(float(len(arguments)))\n",
    "\n",
    "    average_sentence_number = 0.0\n",
    "    average_sentence_length = 0.0\n",
    "    average_sentence_sub = 0.0\n",
    "    average_sentence_polar = 0.0\n",
    "\n",
    "    opinion_count = 0\n",
    "    all_text = \"\"\n",
    "    \n",
    "    for opinion in arguments:\n",
    "        opinion_count += 1\n",
    "        sent_number, sent_length, sent_sub, sent_polar = language_feature_generator(opinion[\"opinion text\"])\n",
    "        average_sentence_number += sent_number\n",
    "        average_sentence_length += sent_length\n",
    "        average_sentence_sub += sent_sub\n",
    "        average_sentence_polar += sent_polar\n",
    "        \n",
    "        all_text += \" \"\n",
    "        all_text += opinion[\"opinion text\"]\n",
    "    \n",
    "    average_sentence_number/opinion_count\n",
    "    average_sentence_length/opinion_count\n",
    "    average_sentence_sub = average_sentence_sub / opinion_count\n",
    "    average_sentence_polar = average_sentence_polar / opinion_count\n",
    "\n",
    "    language_vec.append(average_sentence_number)\n",
    "    language_vec.append(average_sentence_length)\n",
    "    language_vec.append(average_sentence_sub)\n",
    "    language_vec.append(average_sentence_polar)\n",
    "    \n",
    "    try:\n",
    "        all = ['0-Assessments', '1-Authority', '2-Causation', '3-Conditionals', '4-Contrast', '5-Difficulty', '6-Doubt', '7-Emphasis',\\\n",
    "         '8-Generalization', '9-Inconsistency', '10-Inyourshoes', '11-Necessity', '12-Possibility', '13-Priority', '14-Rhetoricalquestion',\\\n",
    "         '15-Structure', '16-Wants']\n",
    "        lexicon_score = arglex.analyse(all_text)\n",
    "        useful_lex = [lexicon_score[1], lexicon_score[3], lexicon_score[4], lexicon_score[5], lexicon_score[11]]\n",
    "        language_vec.extend(useful_lex)\n",
    "        # print(useful_lex)\n",
    "    except:\n",
    "        language_vec.extend([0.0 for i in range(5)])\n",
    "    \n",
    "    \n",
    "    while len(language_vec) != 9:\n",
    "# #         print(user[\"opinion_arguments\"])\n",
    "# #         print(useful_lex)\n",
    "# #         print(language_vec)\n",
    "        language_vec.append(0.0)\n",
    "    \n",
    "    return language_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of user language vector [0.0, 0.0, 0.0, 0.0, 0.14286, 14.0, 55.63333333333333, 0.24047138047138047, -0.01085858585858585]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "target_big_issue = [\"Drug Legalization\", \"Medical Marijuana\"]#,\"Gay Marriage\"]\n",
    "\n",
    "my_user = users[\"ahuggies30\"]\n",
    "my_user = users[\"yomama12\"]\n",
    "# print(\"Example of user aspect vector: \", get_user_aspect_vec(my_user, criterion, language_model, target_big_issue))\n",
    "print(\"Example of user language vector\", lenget_user_language_vec(my_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set(users, target_big_issue, big_issue_list, criterion, w2v_model):\n",
    "    xTr = []\n",
    "    yTr = []\n",
    "    # possible = [\"Pro\", \"Con\", \"N/O\", \"N/S\", \"Und\"]\n",
    "    possible = [\"Pro\", \"Con\", \"N/O\", \"Und\"]\n",
    "    see = dict()\n",
    "    for i in possible:\n",
    "        see[i] = 0\n",
    "\n",
    "    for user in users:\n",
    "        vec = get_user_aspect_vec(user, criterion, w2v_model, target_big_issue)\n",
    "        label = []\n",
    "        for target in target_big_issue:\n",
    "            if user[\"big_issues_dict\"][target] == \"N/S\":\n",
    "                continue\n",
    "            elif user[\"big_issues_dict\"][target] == \"N/O\" or user[\"big_issues_dict\"][target] == \"Und\":\n",
    "                continue\n",
    "                label.extend([0.0, 1.0])\n",
    "            else:\n",
    "                # mask = [0.0, 0.0, 0.0, 0.0]\n",
    "                mask = [0.0, 0.0]\n",
    "                mask[possible.index(user[\"big_issues_dict\"][target])] = 1.0\n",
    "                label.extend(mask)\n",
    "\n",
    "        # see[possible[label]] += 1\n",
    "\n",
    "        vec.extend(get_user_language_vec(user))\n",
    "\n",
    "        if vec and len(label) == 2 * len(target_big_issue):\n",
    "            # no N/S\n",
    "            xTr.append(vec)\n",
    "            yTr.append(label)\n",
    "\n",
    "    return xTr, yTr\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_dim=64, out_dim=2, output_size=3):\n",
    "        super(model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.embeds = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.encoder = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim, out_dim * output_size)\n",
    "\n",
    "        self.loss = nn.BCELoss(reduction='mean')\n",
    "        # self.loss = nn.MultiLabelMarginLoss()\n",
    "        \n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.indicator = 1\n",
    "\n",
    "    def compute_loss(self, pred_vec, gold_seq):\n",
    "        \n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         loss = 0\n",
    "#         for lx in range(len(pred_vec))\n",
    "        \n",
    "        return self.loss(pred_vec, gold_seq)\n",
    "\n",
    "    def forward(self, input_vectors):\n",
    "        #         input_vectors = self.embeds(torch.tensor(input_seq))\n",
    "\n",
    "        input_vectors = self.embeds(input_vectors)\n",
    "        # hidden = input_vectors\n",
    "        # input_vectors = input_vectors.unsqueeze(1)\n",
    "        # _, hidden = self.encoder(input_vectors)\n",
    "        # print(output.size())\n",
    "        input_vectors = torch.nn.functional.leaky_relu(input_vectors, negative_slope=0.01, inplace=False)\n",
    "        hidden = self.encoder(input_vectors)\n",
    "        hidden = torch.nn.functional.leaky_relu(hidden, negative_slope=0.01, inplace=False)\n",
    "        hidden = self.encoder(hidden)\n",
    "        output = torch.nn.functional.leaky_relu(hidden, negative_slope=0.01, inplace=False)\n",
    "        output = self.out(output)\n",
    "        predictions = torch.sigmoid(output)\n",
    "        predictions = predictions.squeeze()\n",
    "\n",
    "        # print(\"predictions:\", predictions)\n",
    "        idxs = []\n",
    "        for i in range(self.output_size):\n",
    "            prediction = predictions[(i * self.out_dim):((i + 1)) * self.out_dim]\n",
    "            prediction = prediction.squeeze()\n",
    "            val, idx = torch.max(prediction, 0)\n",
    "            # idx = idx.item\n",
    "            if idx == 0:\n",
    "                # idxs.extend([1.0, 0.0, 0.0, 0.0])\n",
    "                idxs.extend([1.0, 0.0])\n",
    "            elif idx == 1:\n",
    "                idxs.extend([0.0, 1.0])\n",
    "                # idxs.extend([0.0, 1.0, 0.0, 0.0])\n",
    "            elif idx == 2:\n",
    "                idxs.extend([0.0, 0.0, 1.0])\n",
    "            elif idx == 3:\n",
    "                idxs.extend([0.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "        return predictions, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53905\n",
      "Word2Vec(vocab=48, size=5, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "################### Find the nearest via Embedding ##########################\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for user in users.values():\n",
    "    user_pro = []\n",
    "    user_con = []\n",
    "    user_und = []\n",
    "    user_no = []\n",
    "    # print(user[\"big_issues_dict\"])\n",
    "    for key in user[\"big_issues_dict\"].keys():\n",
    "        if user[\"big_issues_dict\"][key] == \"Pro\":\n",
    "            user_pro.append(key)\n",
    "        if user[\"big_issues_dict\"][key] == \"Con\":\n",
    "            user_con.append(key)\n",
    "        if user[\"big_issues_dict\"][key] == \"Und\":\n",
    "            user_und.append(key)\n",
    "        if user[\"big_issues_dict\"][key] == \"N/O\":\n",
    "            user_no.append(key)\n",
    "\n",
    "    if len(user_pro) > 0:\n",
    "        sentences.append(user_pro)\n",
    "    if len(user_con) > 0:\n",
    "        sentences.append(user_con)\n",
    "    if len(user_und) > 0:\n",
    "        sentences.append(user_und)\n",
    "    if len(user_no) > 0:\n",
    "        sentences.append(user_no)\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "bi_model = Word2Vec(sentences, size=5, window=50)\n",
    "print(bi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhao\\Anaconda3\\envs\\4901\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abortion': ['Drug Legalization', 'National Health Care', 'Civil Unions', 'Global Warming Exists', 'United Nations', 'Welfare', 'Environmental Protection', 'Minimum Wage', 'Social Security', 'Smoking Ban'], 'Animal Rights': ['Euthanasia', 'Legalized Prostitution', 'Medical Marijuana', 'Environmental Protection', 'Global Warming Exists', 'Medicaid & Medicare', 'Minimum Wage', 'Civil Unions', 'Drug Legalization', 'Labor Union'], 'Barack Obama': ['Socialism', 'Globalization', 'Estate Tax', 'Free Trade', 'National Retail Sales Tax', 'Affirmative Action', 'Progressive Tax', 'Social Programs', 'Euthanasia', 'Term Limits'], 'Capitalism': ['Free Trade', 'Homeschooling', 'Globalization', 'Barack Obama', 'Gold Standard', 'Legalized Prostitution', 'Socialism', 'Electoral College', 'Affirmative Action', 'Federal Reserve'], 'Environmental Protection': ['Global Warming Exists', 'Medical Marijuana', 'Drug Legalization', 'Animal Rights', 'Minimum Wage', 'Medicaid & Medicare', 'United Nations', 'Civil Unions', 'Euthanasia', 'Welfare'], 'European Union': ['Occupy Movement', 'Stimulus Spending', 'Redistribution', 'Federal Reserve', 'Globalization', 'Affirmative Action', 'Legalized Prostitution', 'Homeschooling', 'Civil Unions', 'Free Trade'], 'Euthanasia': ['Medicaid & Medicare', 'Animal Rights', 'Social Programs', 'Labor Union', 'Term Limits', 'Legalized Prostitution', 'Minimum Wage', 'Medical Marijuana', 'Social Security', 'Free Trade'], 'Federal Reserve': ['Redistribution', 'Stimulus Spending', 'Occupy Movement', 'Flat Tax', 'Affirmative Action', 'European Union', 'Gold Standard', 'National Retail Sales Tax', 'Socialism', 'Electoral College'], 'Free Trade': ['Globalization', 'Barack Obama', 'Legalized Prostitution', 'Capitalism', 'Socialism', 'Affirmative Action', 'Estate Tax', 'Euthanasia', 'Homeschooling', 'Stimulus Spending'], 'Gay Marriage': ['Medical Marijuana', 'Environmental Protection', 'Euthanasia', 'Social Programs', 'Animal Rights', 'Medicaid & Medicare', 'Global Warming Exists', 'Term Limits', 'Minimum Wage', 'Labor Union'], 'Global Warming Exists': ['Environmental Protection', 'Drug Legalization', 'Minimum Wage', 'Civil Unions', 'United Nations', 'Medical Marijuana', 'National Health Care', 'Welfare', 'Animal Rights', 'Medicaid & Medicare'], 'Globalization': ['Free Trade', 'Barack Obama', 'Socialism', 'Affirmative Action', 'Estate Tax', 'Stimulus Spending', 'Redistribution', 'Legalized Prostitution', 'Progressive Tax', 'European Union'], 'Labor Union': ['Medicaid & Medicare', 'Social Security', 'Term Limits', 'Minimum Wage', 'Welfare', 'United Nations', 'Social Programs', 'Euthanasia', 'Progressive Tax', 'Civil Unions'], 'Legalized Prostitution': ['Free Trade', 'Euthanasia', 'Animal Rights', 'Globalization', 'Civil Unions', 'Labor Union', 'European Union', 'Homeschooling', 'Term Limits', 'Minimum Wage'], 'Medical Marijuana': ['Environmental Protection', 'Gay Marriage', 'Animal Rights', 'Euthanasia', 'Global Warming Exists', 'Medicaid & Medicare', 'Minimum Wage', 'Social Programs', 'Drug Legalization', 'Term Limits'], 'Military Intervention': ['Electoral College', 'Internet Censorship', 'Gold Standard', 'Flat Tax', 'Racial Profiling', 'Border Fence', 'Federal Reserve', 'National Retail Sales Tax', 'Redistribution', 'Stimulus Spending'], 'Minimum Wage': ['United Nations', 'Social Security', 'Welfare', 'Medicaid & Medicare', 'Labor Union', 'Civil Unions', 'National Health Care', 'Term Limits', 'Global Warming Exists', 'Euthanasia'], 'National Health Care': ['Civil Unions', 'United Nations', 'Welfare', 'Social Security', 'Minimum Wage', 'Global Warming Exists', 'Labor Union', 'Abortion', 'Medicaid & Medicare', 'Smoking Ban'], 'Progressive Tax': ['Affirmative Action', 'National Retail Sales Tax', 'Estate Tax', 'Labor Union', 'Socialism', 'Redistribution', 'Stimulus Spending', 'Term Limits', 'Occupy Movement', 'Globalization'], 'Redistribution': ['Stimulus Spending', 'Occupy Movement', 'Federal Reserve', 'Affirmative Action', 'European Union', 'Progressive Tax', 'National Retail Sales Tax', 'Socialism', 'Globalization', 'Flat Tax'], 'Social Programs': ['Medicaid & Medicare', 'Term Limits', 'Labor Union', 'Euthanasia', 'Minimum Wage', 'Social Security', 'Progressive Tax', 'Estate Tax', 'Barack Obama', 'Welfare'], 'Social Security': ['Welfare', 'United Nations', 'Minimum Wage', 'Labor Union', 'Medicaid & Medicare', 'National Health Care', 'Civil Unions', 'Term Limits', 'Progressive Tax', 'Social Programs'], 'Stimulus Spending': ['Redistribution', 'Occupy Movement', 'Federal Reserve', 'Affirmative Action', 'European Union', 'Progressive Tax', 'Socialism', 'Globalization', 'National Retail Sales Tax', 'Flat Tax'], 'Term Limits': ['Labor Union', 'Medicaid & Medicare', 'Social Programs', 'Minimum Wage', 'Social Security', 'Euthanasia', 'Progressive Tax', 'United Nations', 'Welfare', 'Affirmative Action'], 'United Nations': ['Welfare', 'Social Security', 'Minimum Wage', 'Civil Unions', 'National Health Care', 'Labor Union', 'Medicaid & Medicare', 'Term Limits', 'Global Warming Exists', 'Smoking Ban'], 'War in Afghanistan': ['Internet Censorship', 'Military Intervention', 'Gold Standard', 'Electoral College', 'Border Fence', 'Racial Profiling', 'National Retail Sales Tax', 'Death Penalty', 'Estate Tax', 'Flat Tax'], 'War on Terror': ['Iran-Iraq War', 'Racial Profiling', 'Border Fence', 'Torture', 'Flat Tax', 'Military Intervention', 'Electoral College', 'Smoking Ban', 'Death Penalty', 'Federal Reserve'], 'Welfare': ['Social Security', 'United Nations', 'Minimum Wage', 'National Health Care', 'Civil Unions', 'Labor Union', 'Medicaid & Medicare', 'Term Limits', 'Global Warming Exists', 'Smoking Ban'], 'Flat Tax': ['Electoral College', 'Federal Reserve', 'Military Intervention', 'Redistribution', 'Racial Profiling', 'Stimulus Spending', 'Border Fence', 'Gold Standard', 'National Retail Sales Tax', 'Affirmative Action'], 'Gold Standard': ['Electoral College', 'Military Intervention', 'Internet Censorship', 'Federal Reserve', 'Socialism', 'National Retail Sales Tax', 'Flat Tax', 'Estate Tax', 'Redistribution', 'Stimulus Spending'], 'Gun Rights': ['Death Penalty', 'Smoking Ban', 'Drug Legalization', 'Homeschooling', 'Abortion', 'Border Fence', 'Civil Unions', 'War on Terror', 'National Health Care', 'Legalized Prostitution'], 'Internet Censorship': ['Military Intervention', 'Gold Standard', 'Electoral College', 'War in Afghanistan', 'Flat Tax', 'Racial Profiling', 'Border Fence', 'National Retail Sales Tax', 'Federal Reserve', 'Socialism'], 'Iran-Iraq War': ['War on Terror', 'Torture', 'Racial Profiling', 'Border Fence', 'Flat Tax', 'Military Intervention', 'Electoral College', 'Federal Reserve', 'Internet Censorship', 'Death Penalty'], 'Occupy Movement': ['Stimulus Spending', 'Redistribution', 'European Union', 'Federal Reserve', 'Affirmative Action', 'Progressive Tax', 'Globalization', 'Socialism', 'National Retail Sales Tax', 'Flat Tax'], 'Smoking Ban': ['Affirmative Action', 'United Nations', 'Term Limits', 'Social Security', 'Welfare', 'Civil Unions', 'Progressive Tax', 'National Health Care', 'Minimum Wage', 'Labor Union'], 'Socialism': ['Estate Tax', 'Barack Obama', 'National Retail Sales Tax', 'Globalization', 'Affirmative Action', 'Progressive Tax', 'Redistribution', 'Stimulus Spending', 'Free Trade', 'Gold Standard'], 'Torture': ['Iran-Iraq War', 'War on Terror', 'Racial Profiling', 'Border Fence', 'Electoral College', 'Flat Tax', 'Military Intervention', 'Internet Censorship', 'Death Penalty', 'Federal Reserve'], 'Death Penalty': ['Border Fence', 'Gun Rights', 'Racial Profiling', 'War on Terror', 'Flat Tax', 'Internet Censorship', 'Military Intervention', 'Electoral College', 'Capitalism', 'Homeschooling'], 'Drug Legalization': ['Global Warming Exists', 'Environmental Protection', 'Civil Unions', 'Abortion', 'Minimum Wage', 'Animal Rights', 'United Nations', 'National Health Care', 'Medical Marijuana', 'Welfare'], 'Electoral College': ['Military Intervention', 'Gold Standard', 'Flat Tax', 'Internet Censorship', 'Federal Reserve', 'Border Fence', 'Racial Profiling', 'Redistribution', 'Stimulus Spending', 'National Retail Sales Tax'], 'Racial Profiling': ['Border Fence', 'War on Terror', 'Iran-Iraq War', 'Flat Tax', 'Military Intervention', 'Torture', 'Electoral College', 'Internet Censorship', 'Death Penalty', 'Gold Standard'], 'Affirmative Action': ['Progressive Tax', 'Redistribution', 'National Retail Sales Tax', 'Stimulus Spending', 'Socialism', 'Estate Tax', 'Occupy Movement', 'Globalization', 'Federal Reserve', 'Term Limits'], 'Border Fence': ['Racial Profiling', 'War on Terror', 'Flat Tax', 'Iran-Iraq War', 'Death Penalty', 'Military Intervention', 'Electoral College', 'Torture', 'Internet Censorship', 'Gold Standard'], 'Civil Unions': ['National Health Care', 'United Nations', 'Welfare', 'Minimum Wage', 'Social Security', 'Labor Union', 'Global Warming Exists', 'Medicaid & Medicare', 'Drug Legalization', 'Legalized Prostitution'], 'Estate Tax': ['Socialism', 'National Retail Sales Tax', 'Barack Obama', 'Progressive Tax', 'Affirmative Action', 'Globalization', 'Redistribution', 'Stimulus Spending', 'Term Limits', 'Free Trade'], 'Homeschooling': ['European Union', 'Capitalism', 'Free Trade', 'Legalized Prostitution', 'Federal Reserve', 'Occupy Movement', 'Affirmative Action', 'Globalization', 'Stimulus Spending', 'Redistribution'], 'Medicaid & Medicare': ['Minimum Wage', 'Labor Union', 'Social Security', 'Social Programs', 'Term Limits', 'Euthanasia', 'Welfare', 'United Nations', 'Civil Unions', 'Medical Marijuana'], 'National Retail Sales Tax': ['Estate Tax', 'Socialism', 'Affirmative Action', 'Progressive Tax', 'Redistribution', 'Stimulus Spending', 'Federal Reserve', 'Barack Obama', 'Gold Standard', 'Globalization']}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words = list(bi_model.wv.vocab)\n",
    "embedding_results = dict()\n",
    "\n",
    "for word in words:\n",
    "    # print(word)\n",
    "    po_list = bi_model.most_similar(positive=[word])\n",
    "    # print(type(list))\n",
    "    # print(model.most_similar(positive=[word]))\n",
    "    \n",
    "    prior_list = []\n",
    "    for item in po_list:\n",
    "        prior_list.append(item[0])\n",
    "        \n",
    "    embedding_results[word] = prior_list\n",
    "    \n",
    "# print(embedding_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drug Legalization', 'National Health Care', 'Civil Unions', 'Global Warming Exists', 'United Nations', 'Welfare', 'Environmental Protection', 'Minimum Wage', 'Social Security', 'Smoking Ban']\n",
      "['Civil Unions' 'Drug Legalization' 'Global Warming Exists'\n",
      " 'United Nations' 'Minimum Wage' 'Welfare' 'Environmental Protection'\n",
      " 'Social Security' 'National Health Care' 'Smoking Ban']\n",
      "['Drug Legalization', 'National Health Care', 'Civil Unions', 'Global Warming Exists', 'United Nations', 'Welfare', 'Environmental Protection', 'Minimum Wage', 'Social Security', 'Smoking Ban']\n"
     ]
    }
   ],
   "source": [
    "print(embedding_results[\"Abortion\"])\n",
    "\n",
    "this = np.copy(embedding_results[\"Abortion\"])\n",
    "np.random.shuffle(this)\n",
    "print(this)\n",
    "\n",
    "print(embedding_results[\"Abortion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/48\n",
      "Abortion\n",
      "single task\n",
      "4648\n",
      "Accuracy:0.656989247311828\n",
      "2/48\n",
      "Affirmative Action\n",
      "single task\n",
      "2294\n",
      "Accuracy:0.5664488017429193\n",
      "3/48\n",
      "Animal Rights\n",
      "single task\n",
      "916\n",
      "Accuracy:0.5380434782608695\n",
      "4/48\n",
      "Barack Obama\n",
      "single task\n",
      "1788\n",
      "Accuracy:0.6284916201117319\n",
      "5/48\n",
      "Border Fence\n",
      "single task\n",
      "3740\n",
      "Accuracy:0.6203208556149733\n",
      "6/48\n",
      "Capitalism\n",
      "single task\n",
      "1212\n",
      "Accuracy:0.5432098765432098\n",
      "7/48\n",
      "Civil Unions\n",
      "single task\n",
      "1910\n",
      "Accuracy:0.6047120418848168\n",
      "8/48\n",
      "Death Penalty\n",
      "single task\n",
      "4594\n",
      "Accuracy:0.5516866158868335\n",
      "9/48\n",
      "Drug Legalization\n",
      "single task\n",
      "4164\n",
      "Accuracy:0.6362545018007203\n",
      "10/48\n",
      "Electoral College\n",
      "single task\n",
      "1422\n",
      "Accuracy:0.5473684210526316\n",
      "11/48\n",
      "Environmental Protection\n",
      "single task\n",
      "1266\n",
      "Accuracy:0.594488188976378\n",
      "12/48\n",
      "Estate Tax\n",
      "single task\n",
      "2166\n",
      "Accuracy:0.543778801843318\n",
      "13/48\n",
      "European Union\n",
      "single task\n",
      "1264\n",
      "Accuracy:0.5059288537549407\n",
      "14/48\n",
      "Euthanasia\n",
      "single task\n",
      "1980\n",
      "Accuracy:0.5732323232323232\n",
      "15/48\n",
      "Federal Reserve\n",
      "single task\n",
      "1174\n",
      "Accuracy:0.5106382978723404\n",
      "16/48\n",
      "Flat Tax\n",
      "single task\n",
      "2608\n",
      "Accuracy:0.5708812260536399\n",
      "17/48\n",
      "Free Trade\n",
      "single task\n",
      "646\n",
      "Accuracy:0.6076923076923076\n",
      "18/48\n",
      "Gay Marriage\n",
      "single task\n",
      "2958\n",
      "Accuracy:0.7162162162162162\n",
      "19/48\n",
      "Global Warming Exists\n",
      "single task\n",
      "2584\n",
      "Accuracy:0.5918762088974855\n",
      "20/48\n",
      "Globalization\n",
      "single task\n",
      "1076\n",
      "Accuracy:0.5648148148148148\n",
      "21/48\n",
      "Gold Standard\n",
      "single task\n",
      "1194\n",
      "Accuracy:0.5564853556485355\n",
      "22/48\n",
      "Gun Rights\n",
      "single task\n",
      "2402\n",
      "Accuracy:0.5613305613305614\n",
      "23/48\n",
      "Homeschooling\n",
      "single task\n",
      "1310\n",
      "Accuracy:0.5305343511450382\n",
      "24/48\n",
      "Internet Censorship\n",
      "single task\n",
      "626\n",
      "Accuracy:0.48412698412698413\n",
      "25/48\n",
      "Iran-Iraq War\n",
      "single task\n",
      "1490\n",
      "Accuracy:0.6677852348993288\n",
      "26/48\n",
      "Labor Union\n",
      "single task\n",
      "2104\n",
      "Accuracy:0.4845605700712589\n",
      "27/48\n",
      "Legalized Prostitution\n",
      "single task\n",
      "1944\n",
      "Accuracy:0.5244215938303342\n",
      "28/48\n",
      "Medicaid & Medicare\n",
      "single task\n",
      "2114\n",
      "Accuracy:0.48936170212765956\n",
      "29/48\n",
      "Medical Marijuana\n",
      "single task\n",
      "1818\n",
      "Accuracy:0.5714285714285714\n",
      "30/48\n",
      "Military Intervention\n",
      "single task\n",
      "1598\n",
      "Accuracy:0.55\n",
      "31/48\n",
      "Minimum Wage\n",
      "single task\n",
      "2376\n",
      "Accuracy:0.634453781512605\n",
      "32/48\n",
      "National Health Care\n",
      "single task\n",
      "3154\n",
      "Accuracy:0.6719492868462758\n",
      "33/48\n",
      "National Retail Sales Tax\n",
      "single task\n",
      "2568\n",
      "Accuracy:0.48249027237354086\n",
      "34/48\n",
      "Occupy Movement\n",
      "single task\n",
      "1242\n",
      "Accuracy:0.6385542168674698\n",
      "35/48\n",
      "Progressive Tax\n",
      "single task\n",
      "2770\n",
      "Accuracy:0.6010830324909747\n",
      "36/48\n",
      "Racial Profiling\n",
      "single task\n",
      "2246\n",
      "Accuracy:0.5644444444444444\n",
      "37/48\n",
      "Redistribution\n",
      "single task\n",
      "1074\n",
      "Accuracy:0.586046511627907\n",
      "38/48\n",
      "Smoking Ban\n",
      "single task\n",
      "3844\n",
      "Accuracy:0.5279583875162549\n",
      "39/48\n",
      "Social Programs\n",
      "single task\n",
      "1854\n",
      "Accuracy:0.5768194070080862\n",
      "40/48\n",
      "Social Security\n",
      "single task\n",
      "1998\n",
      "Accuracy:0.4825\n",
      "41/48\n",
      "Socialism\n",
      "single task\n",
      "1512\n",
      "Accuracy:0.49834983498349833\n",
      "42/48\n",
      "Stimulus Spending\n",
      "single task\n",
      "1036\n",
      "Accuracy:0.5625\n",
      "43/48\n",
      "Term Limits\n",
      "single task\n",
      "1510\n",
      "Accuracy:0.4900662251655629\n",
      "44/48\n",
      "Torture\n",
      "single task\n",
      "1272\n",
      "Accuracy:0.5843137254901961\n",
      "45/48\n",
      "United Nations\n",
      "single task\n",
      "2162\n",
      "Accuracy:0.5519630484988453\n",
      "46/48\n",
      "War in Afghanistan\n",
      "single task\n",
      "2710\n",
      "Accuracy:0.5940959409594095\n",
      "47/48\n",
      "War on Terror\n",
      "single task\n",
      "4070\n",
      "Accuracy:0.5823095823095823\n",
      "48/48\n",
      "Welfare\n",
      "single task\n",
      "3184\n",
      "Accuracy:0.6279434850863422\n",
      "{'Abortion': 0.656989247311828, 'Affirmative Action': 0.5664488017429193, 'Animal Rights': 0.5380434782608695, 'Barack Obama': 0.6284916201117319, 'Border Fence': 0.6203208556149733, 'Capitalism': 0.5432098765432098, 'Civil Unions': 0.6047120418848168, 'Death Penalty': 0.5516866158868335, 'Drug Legalization': 0.6362545018007203, 'Electoral College': 0.5473684210526316, 'Environmental Protection': 0.594488188976378, 'Estate Tax': 0.543778801843318, 'European Union': 0.5059288537549407, 'Euthanasia': 0.5732323232323232, 'Federal Reserve': 0.5106382978723404, 'Flat Tax': 0.5708812260536399, 'Free Trade': 0.6076923076923076, 'Gay Marriage': 0.7162162162162162, 'Global Warming Exists': 0.5918762088974855, 'Globalization': 0.5648148148148148, 'Gold Standard': 0.5564853556485355, 'Gun Rights': 0.5613305613305614, 'Homeschooling': 0.5305343511450382, 'Internet Censorship': 0.48412698412698413, 'Iran-Iraq War': 0.6677852348993288, 'Labor Union': 0.4845605700712589, 'Legalized Prostitution': 0.5244215938303342, 'Medicaid & Medicare': 0.48936170212765956, 'Medical Marijuana': 0.5714285714285714, 'Military Intervention': 0.55, 'Minimum Wage': 0.634453781512605, 'National Health Care': 0.6719492868462758, 'National Retail Sales Tax': 0.48249027237354086, 'Occupy Movement': 0.6385542168674698, 'Progressive Tax': 0.6010830324909747, 'Racial Profiling': 0.5644444444444444, 'Redistribution': 0.586046511627907, 'Smoking Ban': 0.5279583875162549, 'Social Programs': 0.5768194070080862, 'Social Security': 0.4825, 'Socialism': 0.49834983498349833, 'Stimulus Spending': 0.5625, 'Term Limits': 0.4900662251655629, 'Torture': 0.5843137254901961, 'United Nations': 0.5519630484988453, 'War in Afghanistan': 0.5940959409594095, 'War on Terror': 0.5823095823095823, 'Welfare': 0.6279434850863422}\n",
      "1/48\n",
      "Abortion\n",
      "['Drug Legalization']\n",
      "3798\n",
      "Accuracy:0.5815789473684211\n",
      "2/48\n",
      "Affirmative Action\n",
      "['Progressive Tax']\n",
      "1338\n",
      "Accuracy:0.5111940298507462\n",
      "3/48\n",
      "Animal Rights\n",
      "['Euthanasia']\n",
      "650\n",
      "Accuracy:0.46923076923076923\n",
      "4/48\n",
      "Barack Obama\n",
      "['Socialism']\n",
      "972\n",
      "Accuracy:0.5948717948717949\n",
      "5/48\n",
      "Border Fence\n",
      "['Racial Profiling']\n",
      "2850\n",
      "Accuracy:0.5263157894736842\n",
      "6/48\n",
      "Capitalism\n",
      "['Free Trade']\n",
      "682\n",
      "Accuracy:0.49635036496350365\n",
      "7/48\n",
      "Civil Unions\n",
      "['National Health Care']\n",
      "1584\n",
      "Accuracy:0.6056782334384858\n",
      "8/48\n",
      "Death Penalty\n",
      "['Border Fence']\n",
      "3084\n",
      "Accuracy:0.4862236628849271\n",
      "9/48\n",
      "Drug Legalization\n",
      "['Global Warming Exists']\n",
      "3144\n",
      "Accuracy:0.5802861685214626\n",
      "10/48\n",
      "Electoral College\n",
      "['Military Intervention']\n",
      "906\n",
      "Accuracy:0.6208791208791209\n",
      "11/48\n",
      "Environmental Protection\n",
      "['Global Warming Exists']\n",
      "1126\n",
      "Accuracy:0.5707964601769911\n",
      "12/48\n",
      "Estate Tax\n",
      "['Socialism']\n",
      "884\n",
      "Accuracy:0.615819209039548\n",
      "13/48\n",
      "European Union\n",
      "['Occupy Movement']\n",
      "920\n",
      "Accuracy:0.5815217391304348\n",
      "14/48\n",
      "Euthanasia\n",
      "['Medicaid & Medicare']\n",
      "1362\n",
      "Accuracy:0.6336996336996337\n",
      "15/48\n",
      "Federal Reserve\n",
      "['Redistribution']\n",
      "786\n",
      "Accuracy:0.4873417721518987\n",
      "16/48\n",
      "Flat Tax\n",
      "['Electoral College']\n",
      "978\n",
      "Accuracy:0.5204081632653061\n",
      "17/48\n",
      "Free Trade\n",
      "['Globalization']\n",
      "490\n",
      "Accuracy:0.5306122448979592\n",
      "18/48\n",
      "Gay Marriage\n",
      "['Medical Marijuana']\n",
      "2334\n",
      "Accuracy:0.6509635974304069\n",
      "19/48\n",
      "Global Warming Exists\n",
      "['Environmental Protection']\n",
      "1906\n",
      "Accuracy:0.6335078534031413\n",
      "20/48\n",
      "Globalization\n",
      "['Free Trade']\n",
      "876\n",
      "Accuracy:0.5795454545454546\n",
      "21/48\n",
      "Gold Standard\n",
      "['Electoral College']\n",
      "932\n",
      "Accuracy:0.5347593582887701\n",
      "22/48\n",
      "Gun Rights\n",
      "['Death Penalty']\n",
      "2144\n",
      "Accuracy:0.5990675990675991\n",
      "23/48\n",
      "Homeschooling\n",
      "['European Union']\n",
      "786\n",
      "Accuracy:0.5189873417721519\n",
      "24/48\n",
      "Internet Censorship\n",
      "['Military Intervention']\n",
      "416\n",
      "Accuracy:0.4523809523809524\n",
      "25/48\n",
      "Iran-Iraq War\n",
      "['War on Terror']\n",
      "1424\n",
      "Accuracy:0.5824561403508772\n",
      "26/48\n",
      "Labor Union\n",
      "['Medicaid & Medicare']\n",
      "1700\n",
      "Accuracy:0.5911764705882353\n",
      "27/48\n",
      "Legalized Prostitution\n",
      "['Free Trade']\n",
      "1188\n",
      "Accuracy:0.5756302521008403\n",
      "28/48\n",
      "Medicaid & Medicare\n",
      "['Minimum Wage']\n",
      "1904\n",
      "Accuracy:0.6482939632545932\n",
      "29/48\n",
      "Medical Marijuana\n",
      "['Environmental Protection']\n",
      "1444\n",
      "Accuracy:0.5155709342560554\n",
      "30/48\n",
      "Military Intervention\n",
      "['Electoral College']\n",
      "1108\n",
      "Accuracy:0.5315315315315315\n",
      "31/48\n",
      "Minimum Wage\n",
      "['United Nations']\n",
      "1900\n",
      "Accuracy:0.6052631578947368\n",
      "32/48\n",
      "National Health Care\n",
      "['Civil Unions']\n",
      "2148\n",
      "Accuracy:0.7116279069767442\n",
      "33/48\n",
      "National Retail Sales Tax\n",
      "['Estate Tax']\n",
      "2030\n",
      "Accuracy:0.5566502463054187\n",
      "34/48\n",
      "Occupy Movement\n",
      "['Stimulus Spending']\n",
      "730\n",
      "Accuracy:0.5958904109589042\n",
      "35/48\n",
      "Progressive Tax\n",
      "['Affirmative Action']\n",
      "2068\n",
      "Accuracy:0.6473429951690821\n",
      "36/48\n",
      "Racial Profiling\n",
      "['Border Fence']\n",
      "1878\n",
      "Accuracy:0.5531914893617021\n",
      "37/48\n",
      "Redistribution\n",
      "['Stimulus Spending']\n",
      "698\n",
      "Accuracy:0.5142857142857142\n",
      "38/48\n",
      "Smoking Ban\n",
      "['Affirmative Action']\n",
      "2478\n",
      "Accuracy:0.5604838709677419\n",
      "39/48\n",
      "Social Programs\n",
      "['Medicaid & Medicare']\n",
      "1552\n",
      "Accuracy:0.5466237942122186\n",
      "40/48\n",
      "Social Security\n",
      "['Welfare']\n",
      "1802\n",
      "Accuracy:0.5429362880886427\n",
      "41/48\n",
      "Socialism\n",
      "['Estate Tax']\n",
      "892\n",
      "Accuracy:0.659217877094972\n",
      "42/48\n",
      "Stimulus Spending\n",
      "['Redistribution']\n",
      "728\n",
      "Accuracy:0.5547945205479452\n",
      "43/48\n",
      "Term Limits\n",
      "['Labor Union']\n",
      "1170\n",
      "Accuracy:0.47863247863247865\n",
      "44/48\n",
      "Torture\n",
      "['Iran-Iraq War']\n",
      "822\n",
      "Accuracy:0.44242424242424244\n",
      "45/48\n",
      "United Nations\n",
      "['Welfare']\n",
      "1820\n",
      "Accuracy:0.5961538461538461\n",
      "46/48\n",
      "War in Afghanistan\n",
      "['Internet Censorship']\n",
      "874\n",
      "Accuracy:0.44571428571428573\n",
      "47/48\n",
      "War on Terror\n",
      "['Iran-Iraq War']\n",
      "2962\n",
      "Accuracy:0.596964586846543\n",
      "48/48\n",
      "Welfare\n",
      "['Social Security']\n",
      "2480\n",
      "Accuracy:0.6310483870967742\n",
      "{'Abortion': 0.5815789473684211, 'Affirmative Action': 0.5111940298507462, 'Animal Rights': 0.46923076923076923, 'Barack Obama': 0.5948717948717949, 'Border Fence': 0.5263157894736842, 'Capitalism': 0.49635036496350365, 'Civil Unions': 0.6056782334384858, 'Death Penalty': 0.4862236628849271, 'Drug Legalization': 0.5802861685214626, 'Electoral College': 0.6208791208791209, 'Environmental Protection': 0.5707964601769911, 'Estate Tax': 0.615819209039548, 'European Union': 0.5815217391304348, 'Euthanasia': 0.6336996336996337, 'Federal Reserve': 0.4873417721518987, 'Flat Tax': 0.5204081632653061, 'Free Trade': 0.5306122448979592, 'Gay Marriage': 0.6509635974304069, 'Global Warming Exists': 0.6335078534031413, 'Globalization': 0.5795454545454546, 'Gold Standard': 0.5347593582887701, 'Gun Rights': 0.5990675990675991, 'Homeschooling': 0.5189873417721519, 'Internet Censorship': 0.4523809523809524, 'Iran-Iraq War': 0.5824561403508772, 'Labor Union': 0.5911764705882353, 'Legalized Prostitution': 0.5756302521008403, 'Medicaid & Medicare': 0.6482939632545932, 'Medical Marijuana': 0.5155709342560554, 'Military Intervention': 0.5315315315315315, 'Minimum Wage': 0.6052631578947368, 'National Health Care': 0.7116279069767442, 'National Retail Sales Tax': 0.5566502463054187, 'Occupy Movement': 0.5958904109589042, 'Progressive Tax': 0.6473429951690821, 'Racial Profiling': 0.5531914893617021, 'Redistribution': 0.5142857142857142, 'Smoking Ban': 0.5604838709677419, 'Social Programs': 0.5466237942122186, 'Social Security': 0.5429362880886427, 'Socialism': 0.659217877094972, 'Stimulus Spending': 0.5547945205479452, 'Term Limits': 0.47863247863247865, 'Torture': 0.44242424242424244, 'United Nations': 0.5961538461538461, 'War in Afghanistan': 0.44571428571428573, 'War on Terror': 0.596964586846543, 'Welfare': 0.6310483870967742}\n",
      "1/48\n",
      "Abortion\n",
      "['Drug Legalization', 'National Health Care']\n",
      "2714\n",
      "Accuracy:0.7237569060773481\n",
      "2/48\n",
      "Affirmative Action\n",
      "['Progressive Tax', 'Redistribution']\n",
      "486\n",
      "Accuracy:0.46938775510204084\n",
      "3/48\n",
      "Animal Rights\n",
      "['Euthanasia', 'Legalized Prostitution']\n",
      "600\n",
      "Accuracy:0.5166666666666667\n",
      "4/48\n",
      "Barack Obama\n",
      "['Socialism', 'Globalization']\n",
      "680\n",
      "Accuracy:0.45588235294117646\n",
      "5/48\n",
      "Border Fence\n",
      "['Racial Profiling', 'War on Terror']\n",
      "2480\n",
      "Accuracy:0.5383064516129032\n",
      "6/48\n",
      "Capitalism\n",
      "['Free Trade', 'Homeschooling']\n",
      "536\n",
      "Accuracy:0.5925925925925926\n",
      "7/48\n",
      "Civil Unions\n",
      "['National Health Care', 'United Nations']\n",
      "1390\n",
      "Accuracy:0.5431654676258992\n",
      "8/48\n",
      "Death Penalty\n",
      "['Border Fence', 'Gun Rights']\n",
      "2572\n",
      "Accuracy:0.5242718446601942\n",
      "9/48\n",
      "Drug Legalization\n",
      "['Global Warming Exists', 'Environmental Protection']\n",
      "2656\n",
      "Accuracy:0.5601503759398496\n",
      "10/48\n",
      "Electoral College\n",
      "['Military Intervention', 'Gold Standard']\n",
      "604\n",
      "Accuracy:0.6115702479338843\n",
      "11/48\n",
      "Environmental Protection\n",
      "['Global Warming Exists', 'Medical Marijuana']\n",
      "1044\n",
      "Accuracy:0.5789473684210527\n",
      "12/48\n",
      "Estate Tax\n",
      "['Socialism', 'National Retail Sales Tax']\n",
      "644\n",
      "Accuracy:0.5193798449612403\n",
      "13/48\n",
      "European Union\n",
      "['Occupy Movement', 'Stimulus Spending']\n",
      "640\n",
      "Accuracy:0.578125\n",
      "14/48\n",
      "Euthanasia\n",
      "['Medicaid & Medicare', 'Animal Rights']\n",
      "538\n",
      "Accuracy:0.49074074074074076\n",
      "15/48\n",
      "Federal Reserve\n",
      "['Redistribution', 'Stimulus Spending']\n",
      "636\n",
      "Accuracy:0.578125\n",
      "16/48\n",
      "Flat Tax\n",
      "['Electoral College', 'Federal Reserve']\n"
     ]
    }
   ],
   "source": [
    "###### coll = []\n",
    "acc_collection = dict()\n",
    "list_len = len(big_issues_list)\n",
    "\n",
    "from random import randint\n",
    "\n",
    "target_list = ['political_ideology', 'education', 'gender', \"religious_ideology\"]\n",
    "\n",
    "# target_list = ['political_ideology','education', 'gender', \"religious_ideology\", \"party\", \"ethnicity\"]\n",
    "valid_users, criterion = user_filter(users, target_list, 5)\n",
    "\n",
    "\n",
    "for k in [1, 2 ,3, 4]:\n",
    "    \n",
    "    acc_collection[k] = dict()\n",
    "    \n",
    "    for i in range(list_len):\n",
    "    \n",
    "#         target = big_issues_list[i]\n",
    "#         print(str(i+1) + \"/\"+ str(list_len))\n",
    "#         print(target)\n",
    "#         target_big_issue = [target]\n",
    "\n",
    "#         # Use all together\n",
    "#         temp = big_issues_list\n",
    "#         temp[0], temp[i] = temp[i], temp[0]\n",
    "#         target_big_issue = temp\n",
    "#         print(str(i) + \"/\"+ str(list_len))\n",
    "#         print(temp[0])\n",
    "\n",
    "        # K\n",
    "\n",
    "        target = big_issues_list[i]\n",
    "        print(str(i+1) + \"/\"+ str(list_len))\n",
    "        print(target)\n",
    "        target_big_issue = [target]\n",
    "        \n",
    "        if k == 1:\n",
    "            print(\"single task\")\n",
    "        else:\n",
    "            aux = embedding_results[target][0:k-1]\n",
    "            target_big_issue.extend(aux)\n",
    "            print(aux)\n",
    "        \n",
    "        \n",
    "        # Rand\n",
    "#         target = big_issues_list[i]\n",
    "#         print(str(i+1) + \"/\"+ str(list_len))\n",
    "#         print(target)\n",
    "#         target_big_issue = [target]\n",
    "    \n",
    "        \n",
    "#         rand_n = 1\n",
    "#         temp = np.copy(embedding_results[target])\n",
    "#         np.random.shuffle(temp)\n",
    "        \n",
    "#         aux = temp[0:rand_n]\n",
    "\n",
    "        \n",
    "#         target_big_issue.extend(aux)\n",
    "#         print(aux)   \n",
    "        \n",
    "        \n",
    "\n",
    "        # valid_users, criterion = user_filter(users, target_list, 200)\n",
    "        unbalanced_xTr, unbalanced_yTr = get_training_set(valid_users, target_big_issue, big_issues_list, criterion, language_model)\n",
    "\n",
    "        xTr = []\n",
    "        yTr = []\n",
    "\n",
    "        number_of_0 = 0\n",
    "        number_of_1 = 0\n",
    "\n",
    "        # Find the smaller set\n",
    "        for i in range(len(unbalanced_xTr)):\n",
    "            if unbalanced_yTr[i][0] == 0:\n",
    "                    number_of_0 += 1\n",
    "            if unbalanced_yTr[i][0] == 1:\n",
    "                    number_of_1 += 1\n",
    "\n",
    "        if number_of_0 > number_of_1:\n",
    "            number_each = number_of_1\n",
    "        else:\n",
    "            number_each = number_of_0\n",
    "\n",
    "        number_of_0 = 0\n",
    "        number_of_1 = 0\n",
    "\n",
    "        for i in range(len(unbalanced_xTr)):\n",
    "            if unbalanced_yTr[i][0] == 0:\n",
    "                if number_of_0 < number_each:\n",
    "                    xTr.append(unbalanced_xTr[i])\n",
    "                    yTr.append(unbalanced_yTr[i])\n",
    "                    number_of_0 += 1\n",
    "            if unbalanced_yTr[i][0] == 1:\n",
    "                if number_of_1 < number_each:\n",
    "                    xTr.append(unbalanced_xTr[i])\n",
    "                    yTr.append(unbalanced_yTr[i])\n",
    "                    number_of_1 += 1\n",
    "\n",
    "    #     print(len(xTr), len(yTr))\n",
    "    #     print(number_of_0, number_of_1)\n",
    "\n",
    "\n",
    "        mask = np.arange(2 * number_each)\n",
    "        np.random.shuffle(mask)\n",
    "\n",
    "        temp_xTr = []\n",
    "        temp_yTr = []\n",
    "\n",
    "        for pick in mask:\n",
    "            temp_xTr.append(xTr[pick])\n",
    "            temp_yTr.append(yTr[pick])\n",
    "\n",
    "        xTr = temp_xTr\n",
    "        yTr = temp_yTr\n",
    "\n",
    "    #     print(len(xTr), len(yTr))\n",
    "    #     print(len(xTr[0]), len(yTr[0]))\n",
    "\n",
    "\n",
    "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        m = model(len(xTr[0]), 64, 2, len(target_big_issue)).to(device)\n",
    "        optimizer = optim.SGD(m.parameters(), lr=0.1)\n",
    "\n",
    "        training_x = xTr[0:int(0.8 * len(xTr))]\n",
    "        training_y = yTr[0:int(0.8 * len(yTr))]\n",
    "        test_x = xTr[int(0.8 * len(xTr)):]\n",
    "        test_y = yTr[int(0.8 * len(yTr)):]\n",
    "        \n",
    "        print(len(yTr))\n",
    "\n",
    "        for epoch in (range(15)):\n",
    "            m.train()\n",
    "#             print('training')\n",
    "            start_train = time.time()\n",
    "            total_loss = 0\n",
    "\n",
    "            total_predictions = 0\n",
    "            correct = 0\n",
    "\n",
    "            for i in range(int(len(training_x)/10)):\n",
    "                # print(torch.cuda.current_device())\n",
    "                predictions = None\n",
    "                gold_outputs = None\n",
    "                loss = 0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                for j in range(10):\n",
    "                    loss = 0\n",
    "\n",
    "#                     if torch.cuda.is_available():\n",
    "#                         gold_output = torch.cuda.FloatTensor(training_y[i + j], device=device)\n",
    "#                         input_seq = torch.cuda.FloatTensor(training_x[i + j], device=device)\n",
    "#                     else:\n",
    "                    gold_output = torch.FloatTensor(training_y[i + j], device=device)\n",
    "                    input_seq = torch.FloatTensor(training_x[i + j], device=device)\n",
    "\n",
    "                    prediction_vec, prediction = m(input_seq)\n",
    "\n",
    "                    total_predictions += 1\n",
    "                    # gold_outputs = []\n",
    "                    if prediction[0:2] == training_y[i + j][0:2]:\n",
    "                        correct += 1\n",
    "                    if predictions is None:\n",
    "                        predictions = [prediction_vec]\n",
    "                        gold_outputs = [gold_output]\n",
    "                    else:\n",
    "                        predictions.append(prediction_vec)\n",
    "                        gold_outputs.append(gold_output)\n",
    "                    #\n",
    "                    # for lx in range(len(gold_output)):\n",
    "                    #     tmp_loss = m.compute_loss(prediction[lx], gold_output[lx])\n",
    "                    #     loss += tmp_loss\n",
    "\n",
    "                # print(torch.stack(gold_outputs))\n",
    "                # print(torch.stack(predictions))\n",
    "                loss = m.compute_loss(torch.stack(predictions), torch.stack(gold_outputs).squeeze())\n",
    "                # loss = loss/len(target_big_issue)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            accuracy = correct / total_predictions\n",
    "#             print(\"training time: {} for epoch {}\".format(time.time() - start_train, epoch))\n",
    "#             print('total loss:{}'.format(total_loss))\n",
    "#             print('training accuracy:{}'.format(accuracy))\n",
    "\n",
    "        m.eval()\n",
    "        predictions = 0\n",
    "        correct = 0\n",
    "        baseline = 0\n",
    "\n",
    "        max_score = 0\n",
    "        for i in range(int(len(test_x))):\n",
    "#             if torch.cuda.is_available():\n",
    "#                 gold_output = torch.cuda.FloatTensor(test_y[i], device=device)\n",
    "#                 input_seq = torch.cuda.FloatTensor(test_x[i], device=device)\n",
    "#             else:\n",
    "            gold_output = torch.tensor(test_y[i], device=device)\n",
    "            input_seq = torch.FloatTensor(test_x[i], device=device)\n",
    "\n",
    "            _, prediction = m(input_seq)\n",
    "\n",
    "            # print(\"prediction\", prediction)\n",
    "            # print(\"test_y\", test_y[i])\n",
    "\n",
    "            if test_y[i][0:2] == prediction[0:2]:\n",
    "                correct += 1\n",
    "            if test_y[i][0:2] == [0.0, 1.0]:\n",
    "                baseline += 1\n",
    "            predictions += 1\n",
    "\n",
    "        accuracy = correct / predictions\n",
    "        \n",
    "        acc_collection[k][target] = accuracy\n",
    "        # baseline = baseline / predictions\n",
    "        assert 0 <= accuracy <= 1\n",
    "        print('Accuracy:{}'.format(accuracy))\n",
    "        # print('Baseline:{}'.format(baseline))\n",
    "\n",
    "    print(acc_collection[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5744383370045895\n"
     ]
    }
   ],
   "source": [
    "np_acc = np.array(list(acc_collection[1].values()))\n",
    "print(np.mean(np_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'Welfare': 0.5886970172684458}, 2: {'Welfare': 0.6683417085427136}, 3: {'Welfare': 0.6996805111821086}}\n"
     ]
    }
   ],
   "source": [
    "print(acc_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_with_language.txt', 'w') as outfile:  \n",
    "    json.dump(acc_collection, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_acc = acc_collection\n",
    "print(acc_collection)\n",
    "print(emb_acc)\n",
    "print(all_emb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/49\n",
      "Abortion\n",
      "Federal Reserve\n",
      "Accuracy:0.6103286384976526\n",
      "2/49\n",
      "Affirmative Action\n",
      "Free Trade\n",
      "Accuracy:0.7371794871794872\n",
      "3/49\n",
      "Animal Rights\n",
      "Environmental Protection\n",
      "Accuracy:0.6390977443609023\n",
      "4/49\n",
      "Barack Obama\n",
      "War on Terror\n",
      "Accuracy:0.7542372881355932\n",
      "5/49\n",
      "Border Fence\n",
      "Torture\n",
      "Accuracy:0.5138461538461538\n",
      "6/49\n",
      "Capitalism\n",
      "Capitalism\n",
      "Accuracy:0.7037037037037037\n",
      "7/49\n",
      "Civil Unions\n",
      "Federal Reserve\n",
      "Accuracy:0.5\n",
      "8/49\n",
      "Death Penalty\n",
      "Animal Rights\n",
      "Accuracy:0.6582278481012658\n",
      "9/49\n",
      "Drug Legalization\n",
      "Social Security\n",
      "Accuracy:0.6396226415094339\n",
      "10/49\n",
      "Electoral College\n",
      "War in Afghanistan\n",
      "Accuracy:0.6043956043956044\n",
      "11/49\n",
      "Environmental Protection\n",
      "Civil Unions\n",
      "Accuracy:0.6530612244897959\n",
      "12/49\n",
      "Estate Tax\n",
      "Military Intervention\n",
      "Accuracy:0.6449704142011834\n",
      "13/49\n",
      "European Union\n",
      "Euthanasia\n",
      "Accuracy:0.6127450980392157\n",
      "14/49\n",
      "Euthanasia\n",
      "Social Security\n",
      "Accuracy:0.6101083032490975\n",
      "15/49\n",
      "Federal Reserve\n",
      "Gun Rights\n",
      "Accuracy:0.6435185185185185\n",
      "16/49\n",
      "Flat Tax\n",
      "United Nations\n",
      "Accuracy:0.5711009174311926\n",
      "17/49\n",
      "Free Trade\n",
      "War on Terror\n",
      "Accuracy:0.45689655172413796\n",
      "18/49\n",
      "Gay Marriage\n",
      "Affirmative Action\n",
      "Accuracy:0.7431693989071039\n",
      "19/49\n",
      "Global Warming Exists\n",
      "War on Terror\n",
      "Accuracy:0.6990740740740741\n",
      "20/49\n",
      "Globalization\n",
      "Global Warming Exists\n",
      "Accuracy:0.6284153005464481\n",
      "21/49\n",
      "Gold Standard\n",
      "Iran-Iraq War\n",
      "Accuracy:0.6402116402116402\n",
      "22/49\n",
      "Gun Rights\n",
      "Environmental Protection\n",
      "Accuracy:0.6309523809523809\n",
      "23/49\n",
      "Homeschooling\n",
      "Internet Censorship\n",
      "Accuracy:0.5648535564853556\n",
      "24/49\n",
      "Internet Censorship\n",
      "National Health Care\n",
      "Accuracy:0.4742268041237113\n",
      "25/49\n",
      "Iran-Iraq War\n",
      "Stimulus Spending\n",
      "Accuracy:0.5625\n",
      "26/49\n",
      "Labor Union\n",
      "Term Limits\n",
      "Accuracy:0.5485714285714286\n",
      "27/49\n",
      "Legalized Prostitution\n",
      "Gun Rights\n",
      "Accuracy:0.658753709198813\n",
      "28/49\n",
      "Medicaid & Medicare\n",
      "Death Penalty\n",
      "Accuracy:0.712\n",
      "29/49\n",
      "Medical Marijuana\n",
      "Animal Rights\n",
      "Accuracy:0.6370967741935484\n",
      "30/49\n",
      "Military Intervention\n",
      "Socialism\n",
      "Accuracy:0.668\n",
      "31/49\n",
      "Minimum Wage\n",
      "War on Terror\n",
      "Accuracy:0.6756756756756757\n",
      "32/49\n",
      "National Health Care\n",
      "Term Limits\n",
      "Accuracy:0.7647058823529411\n",
      "33/49\n",
      "National Retail Sales Tax\n",
      "Electoral College\n",
      "Accuracy:0.5243243243243243\n",
      "34/49\n",
      "Occupy Movement\n",
      "Legalized Prostitution\n",
      "Accuracy:0.5853658536585366\n",
      "35/49\n",
      "Progressive Tax\n",
      "Iran-Iraq War\n",
      "Accuracy:0.6732456140350878\n",
      "36/49\n",
      "Racial Profiling\n",
      "National Retail Sales Tax\n",
      "Accuracy:0.5854430379746836\n",
      "37/49\n",
      "Redistribution\n",
      "Homeschooling\n",
      "Accuracy:0.75625\n",
      "38/49\n",
      "Smoking Ban\n",
      "Abortion\n",
      "Accuracy:0.4750378214826021\n",
      "39/49\n",
      "Social Programs\n",
      "Medical Marijuana\n",
      "Accuracy:0.781437125748503\n",
      "40/49\n",
      "Social Security\n",
      "Redistribution\n",
      "Accuracy:0.59375\n",
      "41/49\n",
      "Socialism\n",
      "National Health Care\n",
      "Accuracy:0.8195488721804511\n",
      "42/49\n",
      "Stimulus Spending\n",
      "Barack Obama\n",
      "Accuracy:0.7197452229299363\n",
      "43/49\n",
      "Term Limits\n",
      "Torture\n",
      "Accuracy:0.5033557046979866\n",
      "44/49\n",
      "Torture\n",
      "Minimum Wage\n",
      "Accuracy:0.6415094339622641\n",
      "45/49\n",
      "United Nations\n",
      "Homeschooling\n",
      "Accuracy:0.6923076923076923\n",
      "46/49\n",
      "War in Afghanistan\n",
      "Civil Unions\n",
      "Accuracy:0.5964467005076142\n",
      "47/49\n",
      "War on Terror\n",
      "Animal Rights\n",
      "Accuracy:0.660958904109589\n",
      "48/49\n",
      "Welfare\n",
      "Racial Profiling\n",
      "Accuracy:0.6910569105691057\n",
      "49/49\n",
      "Iran-Iraq War\n",
      "Medicaid & Medicare\n",
      "Accuracy:0.7194570135746606\n",
      "{'Abortion': 0.6103286384976526, 'Affirmative Action': 0.7371794871794872, 'Animal Rights': 0.6390977443609023, 'Barack Obama': 0.7542372881355932, 'Border Fence': 0.5138461538461538, 'Capitalism': 0.7037037037037037, 'Civil Unions': 0.5, 'Death Penalty': 0.6582278481012658, 'Drug Legalization': 0.6396226415094339, 'Electoral College': 0.6043956043956044, 'Environmental Protection': 0.6530612244897959, 'Estate Tax': 0.6449704142011834, 'European Union': 0.6127450980392157, 'Euthanasia': 0.6101083032490975, 'Federal Reserve': 0.6435185185185185, 'Flat Tax': 0.5711009174311926, 'Free Trade': 0.45689655172413796, 'Gay Marriage': 0.7431693989071039, 'Global Warming Exists': 0.6990740740740741, 'Globalization': 0.6284153005464481, 'Gold Standard': 0.6402116402116402, 'Gun Rights': 0.6309523809523809, 'Homeschooling': 0.5648535564853556, 'Internet Censorship': 0.4742268041237113, 'Iran-Iraq War': 0.7194570135746606, 'Labor Union': 0.5485714285714286, 'Legalized Prostitution': 0.658753709198813, 'Medicaid & Medicare': 0.712, 'Medical Marijuana': 0.6370967741935484, 'Military Intervention': 0.668, 'Minimum Wage': 0.6756756756756757, 'National Health Care': 0.7647058823529411, 'National Retail Sales Tax': 0.5243243243243243, 'Occupy Movement': 0.5853658536585366, 'Progressive Tax': 0.6732456140350878, 'Racial Profiling': 0.5854430379746836, 'Redistribution': 0.75625, 'Smoking Ban': 0.4750378214826021, 'Social Programs': 0.781437125748503, 'Social Security': 0.59375, 'Socialism': 0.8195488721804511, 'Stimulus Spending': 0.7197452229299363, 'Term Limits': 0.5033557046979866, 'Torture': 0.6415094339622641, 'United Nations': 0.6923076923076923, 'War in Afghanistan': 0.5964467005076142, 'War on Terror': 0.660958904109589, 'Welfare': 0.6910569105691057}\n"
     ]
    }
   ],
   "source": [
    "# coll = []\n",
    "# acc_collection = dict()\n",
    "# list_len = len(big_issues_list)\n",
    "\n",
    "# target_list = ['political_ideology', 'education', 'gender', \"religious_ideology\"]\n",
    "\n",
    "# # target_list = ['political_ideology','education', 'gender', \"religious_ideology\", \"party\", \"ethnicity\"]\n",
    "# valid_users, criterion = user_filter(users, target_list, 5)\n",
    "\n",
    "# for k in [1]:\n",
    "#     for i in range(list_len):\n",
    "    \n",
    "# #     target = big_issues_list[i]\n",
    "# #     print(str(i) + \"/\"+ str(list_len))\n",
    "# #     print(target)\n",
    "# #     target_big_issue = [target]\n",
    "    \n",
    "# #     # Use all together\n",
    "# #     temp = big_issues_list\n",
    "# #     temp[0], temp[i] = temp[i], temp[0]\n",
    "# #     target_big_issue = temp\n",
    "# #     print(str(i) + \"/\"+ str(list_len))\n",
    "# #     print(temp[0])\n",
    "\n",
    "# #     # K\n",
    "\n",
    "    \n",
    "# #         # k = 1\n",
    "# #         target = big_issues_list[i]\n",
    "# #         print(str(i+1) + \"/\"+ str(list_len))\n",
    "# #         print(target)\n",
    "# #         target_big_issue = [target]\n",
    "\n",
    "# #         aux = embedding_results[target]\n",
    "# #         target_big_issue.extend(aux)\n",
    "# #         print(aux)   \n",
    "        \n",
    "#         # Rand\n",
    "#         target = big_issues_list[i]\n",
    "#         print(str(i+1) + \"/\"+ str(list_len))\n",
    "#         print(target)\n",
    "#         target_big_issue = [target]\n",
    "    \n",
    "        \n",
    "#         rand_n = 1\n",
    "#         temp = np.copy(big_issues_list)\n",
    "#         np.random.shuffle(temp)\n",
    "        \n",
    "#         aux = temp[12]\n",
    "        \n",
    "#         while aux in embedding_results[target]:\n",
    "#             temp = np.copy(big_issues_list)\n",
    "#             np.random.shuffle(temp)\n",
    "#             aux = temp[12]\n",
    "\n",
    "        \n",
    "#         target_big_issue.append(aux)\n",
    "#         print(aux)  \n",
    "        \n",
    "\n",
    "#         # valid_users, criterion = user_filter(users, target_list, 200)\n",
    "#         unbalanced_xTr, unbalanced_yTr = get_training_set(valid_users, target_big_issue, big_issues_list, criterion, language_model)\n",
    "\n",
    "#         xTr = []\n",
    "#         yTr = []\n",
    "\n",
    "#         number_of_0 = 0\n",
    "#         number_of_1 = 0\n",
    "\n",
    "#         # Find the smaller set\n",
    "#         for i in range(len(unbalanced_xTr)):\n",
    "#             if unbalanced_yTr[i][0] == 0:\n",
    "#                     number_of_0 += 1\n",
    "#             if unbalanced_yTr[i][0] == 1:\n",
    "#                     number_of_1 += 1\n",
    "\n",
    "#         if number_of_0 > number_of_1:\n",
    "#             number_each = number_of_1\n",
    "#         else:\n",
    "#             number_each = number_of_0\n",
    "\n",
    "#         number_of_0 = 0\n",
    "#         number_of_1 = 0\n",
    "\n",
    "#         for i in range(len(unbalanced_xTr)):\n",
    "#             if unbalanced_yTr[i][0] == 0:\n",
    "#                 if number_of_0 < number_each:\n",
    "#                     xTr.append(unbalanced_xTr[i])\n",
    "#                     yTr.append(unbalanced_yTr[i])\n",
    "#                     number_of_0 += 1\n",
    "#             if unbalanced_yTr[i][0] == 1:\n",
    "#                 if number_of_1 < number_each:\n",
    "#                     xTr.append(unbalanced_xTr[i])\n",
    "#                     yTr.append(unbalanced_yTr[i])\n",
    "#                     number_of_1 += 1\n",
    "\n",
    "#     #     print(len(xTr), len(yTr))\n",
    "#     #     print(number_of_0, number_of_1)\n",
    "\n",
    "\n",
    "#         mask = np.arange(2 * number_each)\n",
    "#         np.random.shuffle(mask)\n",
    "\n",
    "#         temp_xTr = []\n",
    "#         temp_yTr = []\n",
    "\n",
    "#         for pick in mask:\n",
    "#             temp_xTr.append(xTr[pick])\n",
    "#             temp_yTr.append(yTr[pick])\n",
    "\n",
    "#         xTr = temp_xTr\n",
    "#         yTr = temp_yTr\n",
    "\n",
    "#     #     print(len(xTr), len(yTr))\n",
    "#     #     print(len(xTr[0]), len(yTr[0]))\n",
    "\n",
    "\n",
    "#         # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         device = torch.device(\"cpu\")\n",
    "\n",
    "#         m = model(len(xTr[0]), 64, 2, len(target_big_issue)).to(device)\n",
    "#         optimizer = optim.SGD(m.parameters(), lr=0.1)\n",
    "\n",
    "#         training_x = xTr[0:int(0.8 * len(xTr))]\n",
    "#         training_y = yTr[0:int(0.8 * len(yTr))]\n",
    "#         test_x = xTr[int(0.8 * len(xTr)):]\n",
    "#         test_y = yTr[int(0.8 * len(yTr)):]\n",
    "\n",
    "#         for epoch in (range(10)):\n",
    "#             m.train()\n",
    "# #             print('training')\n",
    "#             start_train = time.time()\n",
    "#             total_loss = 0\n",
    "\n",
    "#             total_predictions = 0\n",
    "#             correct = 0\n",
    "\n",
    "#             for i in range(int(len(training_x)/10)):\n",
    "#                 # print(torch.cuda.current_device())\n",
    "#                 predictions = None\n",
    "#                 gold_outputs = None\n",
    "#                 loss = 0\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 for j in range(10):\n",
    "#                     loss = 0\n",
    "\n",
    "# #                     if torch.cuda.is_available():\n",
    "# #                         gold_output = torch.cuda.FloatTensor(training_y[i + j], device=device)\n",
    "# #                         input_seq = torch.cuda.FloatTensor(training_x[i + j], device=device)\n",
    "# #                     else:\n",
    "#                     gold_output = torch.FloatTensor(training_y[i + j], device=device)\n",
    "#                     input_seq = torch.FloatTensor(training_x[i + j], device=device)\n",
    "\n",
    "#                     prediction_vec, prediction = m(input_seq)\n",
    "\n",
    "#                     total_predictions += 1\n",
    "#                     # gold_outputs = []\n",
    "#                     if prediction[0:2] == training_y[i + j][0:2]:\n",
    "#                         correct += 1\n",
    "#                     if predictions is None:\n",
    "#                         predictions = [prediction_vec]\n",
    "#                         gold_outputs = [gold_output]\n",
    "#                     else:\n",
    "#                         predictions.append(prediction_vec)\n",
    "#                         gold_outputs.append(gold_output)\n",
    "#                     #\n",
    "#                     # for lx in range(len(gold_output)):\n",
    "#                     #     tmp_loss = m.compute_loss(prediction[lx], gold_output[lx])\n",
    "#                     #     loss += tmp_loss\n",
    "\n",
    "#                 # print(torch.stack(gold_outputs))\n",
    "#                 # print(torch.stack(predictions))\n",
    "#                 loss = m.compute_loss(torch.stack(predictions), torch.stack(gold_outputs).squeeze())\n",
    "#                 # loss = loss/len(target_big_issue)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#             accuracy = correct / total_predictions\n",
    "# #             print(\"training time: {} for epoch {}\".format(time.time() - start_train, epoch))\n",
    "# #             print('total loss:{}'.format(total_loss))\n",
    "# #             print('training accuracy:{}'.format(accuracy))\n",
    "\n",
    "#         m.eval()\n",
    "#         predictions = 0\n",
    "#         correct = 0\n",
    "#         baseline = 0\n",
    "\n",
    "#         max_score = 0\n",
    "#         for i in range(int(len(test_x))):\n",
    "# #             if torch.cuda.is_available():\n",
    "# #                 gold_output = torch.cuda.FloatTensor(test_y[i], device=device)\n",
    "# #                 input_seq = torch.cuda.FloatTensor(test_x[i], device=device)\n",
    "# #             else:\n",
    "#             gold_output = torch.tensor(test_y[i], device=device)\n",
    "#             input_seq = torch.FloatTensor(test_x[i], device=device)\n",
    "\n",
    "#             _, prediction = m(input_seq)\n",
    "\n",
    "#             # print(\"prediction\", prediction)\n",
    "#             # print(\"test_y\", test_y[i])\n",
    "\n",
    "#             if test_y[i][0:2] == prediction[0:2]:\n",
    "#                 correct += 1\n",
    "#             if test_y[i][0:2] == [0.0, 1.0]:\n",
    "#                 baseline += 1\n",
    "#             predictions += 1\n",
    "\n",
    "#         accuracy = correct / predictions\n",
    "#         acc_collection[target] = accuracy\n",
    "#         # baseline = baseline / predictions\n",
    "#         assert 0 <= accuracy <= 1\n",
    "#         print('Accuracy:{}'.format(accuracy))\n",
    "#         # print('Baseline:{}'.format(baseline))\n",
    "\n",
    "#     print(acc_collection)\n",
    "#     coll.append(acc_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
